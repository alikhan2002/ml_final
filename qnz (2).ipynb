{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48879948-c0de-42aa-ac88-e5c89132336b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['http_proxy'] = \"http://proxy-ws.cbank.kz:8080\"\n",
    "os.environ['https_proxy'] = \"http://proxy-ws.cbank.kz:8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "affe2ccd-555b-4a86-aa6c-59b354f6306c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, AutoTokenizer, AutoModel\n",
    "from transformers import AutoModelForTokenClassification\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "import onnxruntime\n",
    "import numpy as np\n",
    "import torch\n",
    "import onnx\n",
    "import time\n",
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85057bd1-2cd9-4ac2-a22a-be86d616817c",
   "metadata": {},
   "source": [
    "### WRITE DOWN MODEL PATH ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5ac2e393-d036-409b-b44f-a566f2fea312",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'main_ner' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c6dc2ada-a95f-4aae-bb7e-e6d4d2f588ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = f\"{model_type}/\"\n",
    "if not os.path.exists(model_path):\n",
    "    assert False, f\"The path '{model_path}' does not exist.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2cd6f93f-9e3b-407d-b459-55795b2df032",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder created: optimized_main_ner/\n"
     ]
    }
   ],
   "source": [
    "model_path_folder = [item for item in model_path.split(\"/\") if item][-1]\n",
    "optimized_folder = \"optimized_\" + model_path_folder\n",
    "index = model_path.rfind(model_path_folder)\n",
    "first_part = model_path[:index]\n",
    "second_part = model_path[index+len(model_path_folder):]\n",
    "new_model_path = first_part + optimized_folder + second_part\n",
    "\n",
    "if os.path.exists(new_model_path):\n",
    "    for filename in os.listdir(new_model_path):\n",
    "        file_path = os.path.join(new_model_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path) or os.path.islink(file_path):\n",
    "                os.unlink(file_path)\n",
    "            elif os.path.isdir(file_path):\n",
    "                shutil.rmtree(file_path)\n",
    "        except Exception as e:\n",
    "            print(f'Failed to delete {file_path}. Reason: {e}')\n",
    "    os.rmdir(new_model_path)\n",
    "    print(f\"Deleted existing folder: {new_model_path}\")\n",
    "\n",
    "os.makedirs(new_model_path, exist_ok=True)\n",
    "print(f\"Folder created: {new_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28feddbd-bfa6-4a6c-bd24-ae5b5f795118",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path) # e5 models\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_path) # e5 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "6a543488-edba-40d1-9e33-d380d01447d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['Менің атым Абылай']\n",
    "encoded_input = tokenizer(sentences,max_length=512, padding=True, truncation=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6e05d7ad-16a7-444a-aafd-c51ec80ec36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_fp32 = new_model_path + 'temp_model.onnx'\n",
    "model_quant = new_model_path + 'model.onnx'\n",
    "\n",
    "# Export ONNX model\n",
    "torch.onnx.export(\n",
    "    model, \n",
    "    f=model_fp32,  \n",
    "    input_names=['input_ids', 'attention_mask'],  \n",
    "    output_names=['logits'],  \n",
    "    dynamic_axes={\n",
    "        'input_ids': {0: 'batch_size', 1: 'sequence'},\n",
    "        'attention_mask': {0: 'batch_size', 1: 'sequence'},\n",
    "        'logits': {0: 'batch_size', 1: 'sequence'} \n",
    "    }, \n",
    "    do_constant_folding=True, \n",
    "    opset_version=14, \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2453cd0d-49d3-4051-892c-f766d2bf913a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
     ]
    }
   ],
   "source": [
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantized_model = quantize_dynamic(\n",
    "    model_fp32, \n",
    "    model_quant,\n",
    "    weight_type=QuantType.QInt8  \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "acd32263-eb9c-4c92-850c-ed1fe4fa8774",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_and_combine_tokens(tokens, labels):\n",
    "    cleaned_tokens = []\n",
    "    cleaned_labels = []\n",
    "    current_word = \"\"\n",
    "    current_label = None\n",
    "\n",
    "    for token, label in zip(tokens, labels):\n",
    "        if token in [\"<s>\", \"</s>\", \"<unk>\"]:  \n",
    "            continue\n",
    "        \n",
    "        if token.startswith(\"▁\"):  \n",
    "            if current_word:  \n",
    "                cleaned_tokens.append(current_word)\n",
    "                cleaned_labels.append(current_label)\n",
    "            current_word = token[1:]  \n",
    "            current_label = label\n",
    "        else:  \n",
    "            current_word += token\n",
    "\n",
    "        if current_label == \"O\":\n",
    "            current_label = label\n",
    "\n",
    "    if current_word:\n",
    "        cleaned_tokens.append(current_word)\n",
    "        cleaned_labels.append(current_label)\n",
    "\n",
    "    return cleaned_tokens, cleaned_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24c11d78-a626-434a-8d2e-91ff20eabdad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: Менің\t Label: O\n",
      "Token: атым\t Label: O\n",
      "Token: Абылай\t Label: B-PERSON\n"
     ]
    }
   ],
   "source": [
    "clean_tokens, clean_labels = clean_and_combine_tokens(tokens, predicted_labels)\n",
    "\n",
    "for token, label in zip(clean_tokens, clean_labels):\n",
    "    print(f\"Token: {token}\\t Label: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a6f4c7a9-9d76-4ae6-8c11-1ea3c80f336d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "df = pd.read_csv('ner_dataset.csv')\n",
    "config = json.load(open(\"main_ner_wlang/config.json\"))\n",
    "id2label = config['id2label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "26adaf3b-090b-4181-be31-22304869ba81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Менің', 'атым', 'Абылай'] ['O', 'O', 'B-PERSON']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def run_ner_inference(input_text):\n",
    "    encoded_input = tokenizer(input_text, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=True)\n",
    "    input_ids = encoded_input['input_ids'].numpy()\n",
    "    attention_mask = encoded_input['attention_mask'].numpy()\n",
    "\n",
    "    ort_inputs = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n",
    "\n",
    "    ort_outs = ort_session.run(['logits'], ort_inputs)\n",
    "    logits = ort_outs[0]\n",
    "\n",
    "    probabilities = F.softmax(torch.from_numpy(logits), dim=-1).numpy()\n",
    "\n",
    "    return probabilities, input_ids\n",
    "\n",
    "ort_session = onnxruntime.InferenceSession(model_quant, providers=[\"CPUExecutionProvider\"])\n",
    "input_text = \"Менің атым Абылай\" \n",
    "probabilities, input_ids = run_ner_inference(input_text)\n",
    "\n",
    "predicted_classes = np.argmax(probabilities, axis=-1)\n",
    "\n",
    "id2label = config['id2label']\n",
    "\n",
    "predicted_labels = [id2label[str(class_id)] for class_id in predicted_classes[0]]\n",
    "\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "clean_tokens, clean_labels = clean_and_combine_tokens(tokens, predicted_labels)\n",
    "print(clean_tokens, clean_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "c0c6c2e5-0943-4efb-bd4c-d78415f53e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "going for ner: 100%|██████████| 8013/8013 [14:55<00:00,  8.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7h 49min 43s, sys: 5 s, total: 7h 49min 48s\n",
      "Wall time: 14min 56s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "ort_session = onnxruntime.InferenceSession(model_quant, providers=[\"CPUExecutionProvider\"])\n",
    "for ind, it in tqdm(df.iterrows(), desc='going for ner', total=len(df)):\n",
    "\n",
    "    input_text = it['content']\n",
    "    logits, input_ids = run_ner_inference(input_text)\n",
    "    \n",
    "    predicted_classes = np.argmax(logits, axis=-1)\n",
    "\n",
    "    predicted_labels = [id2label[str(class_id)] for class_id in predicted_classes[0]]\n",
    "    \n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "    \n",
    "    \n",
    "    clean_tokens, clean_labels = clean_and_combine_tokens(tokens, predicted_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
